{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8c1c42806ccd7bc",
   "metadata": {
    "id": "c8c1c42806ccd7bc"
   },
   "source": [
    "# **Pirate Pain Challenge - Hyperparameters Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47a74d0babd730",
   "metadata": {
    "id": "bf47a74d0babd730"
   },
   "source": [
    "## üåê **Google Drive Connection or local mount**"
   ]
  },
  {
   "cell_type": "code",
   "id": "f644bab597852f40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f644bab597852f40",
    "outputId": "8b184930-243e-45a3-c73e-8b8961375221",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:24.520802Z",
     "start_time": "2025-11-14T01:31:24.507336Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "isColab = False\n",
    "isKaggle = False\n",
    "\n",
    "# Directory di default\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "try:\n",
    "    if not isColab:\n",
    "        raise ImportError(\"We are not in google colab\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/gdrive\")\n",
    "    current_dir = \"/gdrive/My\\\\ Drive/[2025-2026]\\\\ AN2DL/AN2DL-challenge-1/\"\n",
    "    print(\"In esecuzione su Colab. Google Drive montato.\")\n",
    "    %cd $current_dir\n",
    "    isColab = True\n",
    "\n",
    "except ImportError:\n",
    "    # Rilevamento ambiente Kaggle\n",
    "    if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") or os.path.exists(\"/kaggle/working\") or isKaggle:\n",
    "        isKaggle = True\n",
    "        kaggle_work_dir = \"/kaggle/working/AN2DL-challenge-1\"\n",
    "        os.makedirs(kaggle_work_dir, exist_ok=True)\n",
    "        current_dir = kaggle_work_dir\n",
    "        print(\"In esecuzione su Kaggle. Directory di lavoro impostata.\")\n",
    "    else:\n",
    "        isColab = False\n",
    "        isKaggle = False\n",
    "        print(\"Esecuzione locale. Salto mount Google Drive.\")\n",
    "        local_pref = r\"G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-1\"\n",
    "        current_dir = local_pref if os.path.isdir(local_pref) else os.getcwd()\n",
    "        print(f\"Directory corrente impostata a: {current_dir}\")\n",
    "\n",
    "# Cambio directory se non Colab (su Colab √® gi√† fatto con %cd)\n",
    "if not isColab:\n",
    "    os.chdir(current_dir)\n",
    "\n",
    "print(f\"Changed directory to: {current_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esecuzione locale. Salto mount Google Drive.\n",
      "Directory corrente impostata a: G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-1\n",
      "Changed directory to: G:\\Il mio Drive\\Colab Notebooks\\[2025-2026] AN2DL\\AN2DL-challenge-1\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "932cdb46ae272e67",
   "metadata": {
    "id": "932cdb46ae272e67"
   },
   "source": [
    "## ‚öôÔ∏è **Libraries Import**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9b8748f0d28fed8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b8748f0d28fed8d",
    "outputId": "6d618f4e-6b90-44c3-8c4c-55fe5d2d489d",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:24.608853Z",
     "start_time": "2025-11-14T01:31:24.551342Z"
    }
   },
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "# from torchsummary import summary\n",
    "\n",
    "logs_dir = \"tensorboard\"\n",
    "if isColab:\n",
    "    !pkill -f tensorboard\n",
    "else:\n",
    "    # Arresta eventuali processi tensorboard in locale (Windows)\n",
    "    import os\n",
    "\n",
    "    if os.name == 'nt':\n",
    "        try:\n",
    "            import psutil\n",
    "\n",
    "            for proc in psutil.process_iter(['name', 'cmdline']):\n",
    "                name = (proc.info.get('name') or '').lower()\n",
    "                cmd = ' '.join(proc.info.get('cmdline') or []).lower()\n",
    "                if 'tensorboard' in name or 'tensorboard' in cmd:\n",
    "                    try:\n",
    "                        proc.kill()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        except ImportError:\n",
    "            import subprocess\n",
    "\n",
    "            subprocess.run(['taskkill', '/F', '/IM', 'tensorboard.exe'],\n",
    "                           stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "%load_ext tensorboard\n",
    "if isColab:\n",
    "    !mkdir -p models\n",
    "else:\n",
    "    os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "PyTorch version: 2.9.0+cu130\n",
      "Device: cuda\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "a8701093d38441af",
   "metadata": {
    "id": "a8701093d38441af"
   },
   "source": [
    "## ‚è≥ **Data Downloading**"
   ]
  },
  {
   "cell_type": "code",
   "id": "1191d5d6f484df56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1191d5d6f484df56",
    "outputId": "d6482cae-e372-4335-8fbd-873ee94d162b",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:24.625856Z",
     "start_time": "2025-11-14T01:31:24.614861Z"
    }
   },
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# --- 1. Impostazioni ---\n",
    "competition_name = 'an2dl2526c1'\n",
    "dataset_path = 'dataset'\n",
    "if isKaggle:\n",
    "    dataset_path = '/kaggle/input/pirate-pain/dataset'\n",
    "train_file = 'pirate_pain_train.csv'\n",
    "test_file = 'pirate_pain_test.csv'\n",
    "labels_file = 'pirate_pain_train_labels.csv'\n",
    "sample_submission_file = 'sample_submission.csv'\n",
    "\n",
    "# Controlla se il dataset √® gi√† stato scaricato ed estratto\n",
    "if not isKaggle and not isColab and not os.path.exists(os.path.join(dataset_path, train_file)):\n",
    "    # --- 2. Autenticazione e Download ---\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    # Inizializza l'API di Kaggle\n",
    "    # L'autenticazione avviene automaticamente se 'kaggle.json' √® in C:\\\\Users\\\\Bert0ns\\\\.kaggle\\\\\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    print(f\"Download del dataset dalla competizione '{competition_name}'...\")\n",
    "\n",
    "    # Crea la directory di destinazione se non esiste\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "\n",
    "    # Scarica i file della competizione nella cartella 'dataset'\n",
    "    api.competition_download_files(competition_name, path=dataset_path)\n",
    "\n",
    "    # Estrai i file dall'archivio zip\n",
    "    zip_path = os.path.join(dataset_path, f'{competition_name}.zip')\n",
    "    if os.path.exists(zip_path):\n",
    "        print(f\"Estrazione dei file da '{zip_path}'...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dataset_path)\n",
    "        # Rimuovi il file zip dopo l'estrazione\n",
    "        os.remove(zip_path)\n",
    "        print(\"Estrazione completata e file zip rimosso.\")\n",
    "    else:\n",
    "        print(\"ATTENZIONE: File zip non trovato. Il download potrebbe non essere riuscito.\")\n",
    "else:\n",
    "    print(f\"Il dataset √® gi√† presente nella cartella {dataset_path}. Download saltato.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il dataset √® gi√† presente nella cartella dataset. Download saltato.\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "id": "f0189d20bfdbdbfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T09:59:15.146847Z",
     "start_time": "2025-11-08T09:59:15.142723Z"
    },
    "id": "f0189d20bfdbdbfc"
   },
   "source": [
    "## üîé **Exploration and Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9d0c6a84418bf8a0",
   "metadata": {
    "id": "9d0c6a84418bf8a0",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:29.463930Z",
     "start_time": "2025-11-14T01:31:24.631866Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "if not isColab:\n",
    "    dataset_df = pd.read_csv(os.path.join(dataset_path, train_file))\n",
    "    kaggle_test_df = pd.read_csv(os.path.join(dataset_path, test_file))\n",
    "    labels_df = pd.read_csv(os.path.join(dataset_path, labels_file))\n",
    "else:\n",
    "    dataset_df = pd.read_csv(dataset_path + '/pirate_pain_train.csv')\n",
    "    kaggle_test_df = pd.read_csv(dataset_path + '/pirate_pain_test.csv')\n",
    "    labels_df = pd.read_csv(dataset_path + '/pirate_pain_train_labels.csv')"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "id": "ab93bdf2d91e1a31",
   "metadata": {
    "id": "ab93bdf2d91e1a31"
   },
   "source": [
    "**Convert data to a memory efficient form**"
   ]
  },
  {
   "cell_type": "code",
   "id": "806aabad03a2142f",
   "metadata": {
    "id": "806aabad03a2142f",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.229196Z",
     "start_time": "2025-11-14T01:31:29.857120Z"
    }
   },
   "source": [
    "\n",
    "text_map = {\n",
    "    'two': 0,\n",
    "    'one+peg_leg': 1, 'one+hook_hand': 2, 'one+eye_patch': 3,\n",
    "}\n",
    "\n",
    "# Pulisce, normalizza, mappa; fallback a numerico e a cifre estratte\n",
    "columns_to_convert = ['n_legs', 'n_hands', 'n_eyes']\n",
    "for col in columns_to_convert:\n",
    "    dataset_df[col] = dataset_df[col].str.strip().str.lower().map(text_map).astype('int8')\n",
    "    kaggle_test_df[col] = kaggle_test_df[col].str.strip().str.lower().map(text_map).astype('int8')\n",
    "\n",
    "# train_df.head(105760)"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "id": "c0e83165934c5ead",
   "metadata": {
    "id": "c0e83165934c5ead",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.364378Z",
     "start_time": "2025-11-14T01:31:30.235216Z"
    }
   },
   "source": [
    "# Convert data types from float64 to float32 to save memory\n",
    "dataset_df[dataset_df.select_dtypes(include=['float64']).columns] = dataset_df.select_dtypes(\n",
    "    include=['float64']).astype(\n",
    "    'float32')\n",
    "kaggle_test_df[kaggle_test_df.select_dtypes(include=['float64']).columns] = kaggle_test_df.select_dtypes(\n",
    "    include=['float64']).astype(\n",
    "    'float32')\n",
    "\n",
    "# Convert int64 to int32\n",
    "dataset_df[dataset_df.select_dtypes(include=['int64']).columns] = dataset_df.select_dtypes(include=['int64']).astype(\n",
    "    'int32')\n",
    "kaggle_test_df[kaggle_test_df.select_dtypes(include=['int64']).columns] = kaggle_test_df.select_dtypes(\n",
    "    include=['int64']).astype('int32')\n",
    "labels_df[labels_df.select_dtypes(include=['int64']).columns] = labels_df.select_dtypes(include=['int64']).astype(\n",
    "    'int32')\n",
    "\n",
    "# Convert pain surveys to int8\n",
    "dataset_df['pain_survey_1'] = dataset_df['pain_survey_1'].astype('int8')\n",
    "dataset_df['pain_survey_2'] = dataset_df['pain_survey_2'].astype('int8')\n",
    "dataset_df['pain_survey_3'] = dataset_df['pain_survey_3'].astype('int8')\n",
    "dataset_df['pain_survey_4'] = dataset_df['pain_survey_4'].astype('int8')\n",
    "\n",
    "kaggle_test_df['pain_survey_1'] = kaggle_test_df['pain_survey_1'].astype('int8')\n",
    "kaggle_test_df['pain_survey_2'] = kaggle_test_df['pain_survey_2'].astype('int8')\n",
    "kaggle_test_df['pain_survey_3'] = kaggle_test_df['pain_survey_3'].astype('int8')\n",
    "kaggle_test_df['pain_survey_4'] = kaggle_test_df['pain_survey_4'].astype('int8')\n",
    "\n",
    "# Convert labels sample_index to int8\n",
    "label_map = {'low_pain': 1, 'no_pain': 0, 'high_pain': 2}\n",
    "\n",
    "labels_df['label'] = labels_df['label'].str.strip().str.lower().map(label_map).astype('int8')"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **Prepare the Time feature**",
   "id": "dfb49b2077a3e588"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.403166Z",
     "start_time": "2025-11-14T01:31:30.369347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Time features: normalized_time + sin/cos ---\n",
    "TIME_FEATURES = ['time_norm', 'time_sin', 'time_cos']\n",
    "# Vengono aggiunte tre nuove colonne continue: time_norm, time_sin, time_cos\n",
    "# Se la sequenza per un sample ha lunghezza variabile, normalizziamo dividendo per il max time per sample.\n",
    "for _df in [dataset_df, kaggle_test_df]:\n",
    "    if 'time' in _df.columns and 'time_norm' not in _df.columns:\n",
    "        max_time = _df.groupby('sample_index')['time'].transform('max').replace(0, 1)\n",
    "        _df['time_norm'] = (_df['time'] / max_time).astype('float32')\n",
    "        _df['time_sin'] = np.sin(2 * np.pi * _df['time_norm']).astype('float32')\n",
    "        _df['time_cos'] = np.cos(2 * np.pi * _df['time_norm']).astype('float32')"
   ],
   "id": "2d75f4faaf0be3ee",
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "id": "3f43311eda52cde4",
   "metadata": {
    "id": "3f43311eda52cde4"
   },
   "source": [
    "## üîÑ **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4a62dc6b39e0396",
   "metadata": {
    "id": "d4a62dc6b39e0396",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.430711Z",
     "start_time": "2025-11-14T01:31:30.423662Z"
    }
   },
   "source": [
    "TEST_SET_PERCENTAGE = 0.2\n",
    "\n",
    "JOINT_COLUMNS = [f'joint_{i:02d}' for i in range(31)]\n",
    "\n",
    "CONTINUOUS_COLS = JOINT_COLUMNS + TIME_FEATURES\n",
    "CATEGORICAL_COLS = ['n_legs', 'n_hands', 'n_eyes', 'pain_survey_1', 'pain_survey_2', 'pain_survey_3', 'pain_survey_4']\n",
    "\n",
    "#COLUMNS_TO_REMOVE = [f'joint_{i:02d}' for i in range(13, 26)] + ['joint_30']\n",
    "COLUMNS_TO_REMOVE = ['joint_30']\n",
    "\n",
    "COLS_TO_EXCLUDE_FROM_NORMALIZATION = TIME_FEATURES"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "295ac2b85394b9c",
   "metadata": {
    "id": "295ac2b85394b9c",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.485243Z",
     "start_time": "2025-11-14T01:31:30.475557Z"
    }
   },
   "source": [
    "num_classes = len(labels_df['label'].unique())\n",
    "unique_samples = dataset_df['sample_index'].unique()"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "id": "add7800109f02509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T10:06:02.823302Z",
     "start_time": "2025-11-08T10:06:02.818073Z"
    },
    "id": "add7800109f02509"
   },
   "source": [
    "#### Remove useless features"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a85d49564acf7ed",
   "metadata": {
    "id": "4a85d49564acf7ed",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.566894Z",
     "start_time": "2025-11-14T01:31:30.513116Z"
    }
   },
   "source": [
    "# @title Remove feature from joint_13 to joint_25 + joint_30\n",
    "df_dataset_reduced = dataset_df.drop(columns=COLUMNS_TO_REMOVE, inplace=False)\n",
    "kaggle_test_df_reduced = kaggle_test_df.drop(columns=COLUMNS_TO_REMOVE, inplace=False)"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "id": "a75b6bdd01a5f9d4",
   "metadata": {
    "id": "a75b6bdd01a5f9d4",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.576385Z",
     "start_time": "2025-11-14T01:31:30.570538Z"
    }
   },
   "source": [
    "# Rimuoviamo le colonne eliminate anche dalle nostre liste di colonne\n",
    "CONTINUOUS_COLS_REDUCED = [col for col in CONTINUOUS_COLS if col not in COLUMNS_TO_REMOVE]\n",
    "CATEGORICAL_COLS_REDUCED = [col for col in CATEGORICAL_COLS if col not in COLUMNS_TO_REMOVE]"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Build sequences with sliding window\n",
    "\n",
    "üéØ **Adaptive Padding Strategy**\n",
    "\n",
    "Invece di usare padding con zeri (che introduce rumore), usiamo **padding adattivo**:\n",
    "- **Continuous features**: Usa la media dell'ultimo N timesteps della sequenza\n",
    "- **Categorical features**: Usa la moda (valore pi√π frequente) dell'ultimo N timesteps\n",
    "- **Fallback**: Se necessario, usa le statistiche globali del dataset\n",
    "\n",
    "Questo approccio riduce il rumore e migliora la qualit√† delle predizioni."
   ],
   "id": "7a9bfb04ec7a1aee"
  },
  {
   "cell_type": "code",
   "id": "9de97e0a807adc2a",
   "metadata": {
    "id": "9de97e0a807adc2a",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.594397Z",
     "start_time": "2025-11-14T01:31:30.580061Z"
    }
   },
   "source": [
    "def build_sequences(df, label_df, continuous_cols, categorical_cols, window=200, stride=200,\n",
    "                    padding_strategy='adaptive', lookback_steps=10):\n",
    "    \"\"\"\n",
    "    Build sequences from time series data with intelligent padding.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with time series data\n",
    "        label_df: DataFrame with labels\n",
    "        continuous_cols: List of continuous feature columns\n",
    "        categorical_cols: List of categorical feature columns\n",
    "        window: Window size for sequences\n",
    "        stride: Stride for sliding window\n",
    "        padding_strategy: 'adaptive' (mean/mode), 'repeat' (repeat last), or 'zero' (zeros)\n",
    "        lookback_steps: Number of timesteps to use for computing padding statistics\n",
    "\n",
    "    Returns:\n",
    "        dataset_continuous, dataset_categorical, labels\n",
    "    \"\"\"\n",
    "    assert window % stride == 0, \"Window must be divisible by stride\"\n",
    "\n",
    "    dataset_continuous = []\n",
    "    dataset_categorical = []\n",
    "    labels = []\n",
    "\n",
    "    # Pre-compute global statistics for fallback (only if adaptive)\n",
    "    if padding_strategy == 'adaptive':\n",
    "        global_cont_mean = df[continuous_cols].mean().values.astype('float32')\n",
    "        global_cat_mode = df[categorical_cols].mode().iloc[0].values.astype('int8')\n",
    "\n",
    "    for sample_id in df['sample_index'].unique():\n",
    "        # Extract data for current sample\n",
    "        temp_continuous = df[df['sample_index'] == sample_id][continuous_cols].values\n",
    "        temp_categorical = df[df['sample_index'] == sample_id][categorical_cols].values\n",
    "\n",
    "        label = label_df[label_df['sample_index'] == sample_id]['label'].values[0]\n",
    "\n",
    "        # Calculate padding length\n",
    "        padding_len = (window - len(temp_continuous) % window) % window\n",
    "\n",
    "        if padding_strategy == 'adaptive':\n",
    "            # Adaptive padding: use statistics from last N timesteps\n",
    "            lookback = min(lookback_steps, len(temp_continuous))\n",
    "\n",
    "            if lookback > 0:\n",
    "                # Use mean of last timesteps for continuous\n",
    "                last_cont_values = temp_continuous[-lookback:]\n",
    "                pad_cont_value = np.mean(last_cont_values, axis=0, keepdims=True)\n",
    "\n",
    "                # Use mode of last timesteps for categorical\n",
    "                last_cat_values = temp_categorical[-lookback:]\n",
    "                pad_cat_value = np.array([\n",
    "                    np.bincount(last_cat_values[:, i]).argmax()\n",
    "                    for i in range(last_cat_values.shape[1])\n",
    "                ]).reshape(1, -1)\n",
    "            else:\n",
    "                # Fallback to global statistics\n",
    "                pad_cont_value = global_cont_mean.reshape(1, -1)\n",
    "                pad_cat_value = global_cat_mode.reshape(1, -1)\n",
    "\n",
    "            # Create padding by repeating the computed values\n",
    "            padding_cont = np.repeat(pad_cont_value, padding_len, axis=0).astype('float32')\n",
    "            padding_cat = np.repeat(pad_cat_value, padding_len, axis=0).astype('int8')\n",
    "        elif padding_strategy == 'repeat':\n",
    "            # Repeat last timestep\n",
    "            if len(temp_continuous) > 0:\n",
    "                padding_cont = np.repeat(temp_continuous[-1:], padding_len, axis=0)\n",
    "                padding_cat = np.repeat(temp_categorical[-1:], padding_len, axis=0)\n",
    "            else:\n",
    "                # Fallback to zeros if no data\n",
    "                padding_cont = np.zeros((padding_len, temp_continuous.shape[1]), dtype='float32')\n",
    "                padding_cat = np.zeros((padding_len, temp_categorical.shape[1]), dtype='int8')\n",
    "        else:  # 'zero' or default\n",
    "            # Original zero padding\n",
    "            padding_cont = np.zeros((padding_len, temp_continuous.shape[1]), dtype='float32')\n",
    "            padding_cat = np.zeros((padding_len, temp_categorical.shape[1]), dtype='int8')\n",
    "\n",
    "        temp_continuous = np.concatenate((temp_continuous, padding_cont))\n",
    "        temp_categorical = np.concatenate((temp_categorical, padding_cat))\n",
    "\n",
    "        # Build windows with sliding stride\n",
    "        idx = 0\n",
    "        while idx + window <= len(temp_continuous):\n",
    "            dataset_continuous.append(temp_continuous[idx:idx + window])\n",
    "            dataset_categorical.append(temp_categorical[idx:idx + window])\n",
    "            labels.append(label)\n",
    "            idx += stride\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    dataset_continuous = np.array(dataset_continuous, dtype='float32')\n",
    "    dataset_categorical = np.array(dataset_categorical, dtype='int8')\n",
    "    labels = np.array(labels, dtype='int64')\n",
    "\n",
    "    return dataset_continuous, dataset_categorical, labels"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "ed8af00de3bc02e6",
   "metadata": {
    "id": "ed8af00de3bc02e6",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.604900Z",
     "start_time": "2025-11-14T01:31:30.597726Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "def make_loader(ds, batch_size, shuffle, drop_last):\n",
    "    # Determine optimal number of worker processes for data loading\n",
    "    cpu_cores = os.cpu_count() or 2\n",
    "    num_workers = max(2, min(4, cpu_cores))\n",
    "\n",
    "    # Create DataLoader with performance optimizations\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
    "        prefetch_factor=4,  # Load 4 batches ahead\n",
    "        persistent_workers=True if num_workers > 0 else False,  # Mantiene i worker attivi\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "id": "484246425afe5f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T10:30:46.123364Z",
     "start_time": "2025-11-08T10:30:46.113836Z"
    },
    "id": "484246425afe5f5f"
   },
   "source": [
    "## üõ†Ô∏è **Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9929d84206ec1de2",
   "metadata": {
    "id": "9929d84206ec1de2",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.625370Z",
     "start_time": "2025-11-14T01:31:30.608313Z"
    }
   },
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def recurrent_summary(model, input_specs):\n",
    "    \"\"\"\n",
    "    Custom summary function that emulates torchinfo's output while correctly\n",
    "    counting parameters for RNN/GRU/LSTM layers. It supports models with multiple inputs.\n",
    "\n",
    "    This function is designed for models whose direct children are\n",
    "    nn.Linear, nn.RNN, nn.GRU, or nn.LSTM layers.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to analyze.\n",
    "        input_specs (list of tuples): A list where each tuple contains the shape\n",
    "                                     and dtype of an input tensor.\n",
    "                                     Example: [((seq_len, features_cont), torch.float32),\n",
    "                                               ((seq_len, features_cat), torch.long)]\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionary to store output shapes captured by forward hooks\n",
    "    output_shapes = {}\n",
    "    # List to track hook handles for later removal\n",
    "    hooks = []\n",
    "\n",
    "    def get_hook(name):\n",
    "        \"\"\"Factory function to create a forward hook for a specific module.\"\"\"\n",
    "\n",
    "        def hook(module, input, output):\n",
    "            # Handle RNN layer outputs (returns a tuple)\n",
    "            if isinstance(output, tuple):\n",
    "                # output[0]: all hidden states with shape (batch, seq_len, hidden*directions)\n",
    "                shape1 = list(output[0].shape)\n",
    "                shape1[0] = -1  # Replace batch dimension with -1\n",
    "\n",
    "                # output[1]: final hidden state h_n (or tuple (h_n, c_n) for LSTM)\n",
    "                if isinstance(output[1], tuple):  # LSTM case: (h_n, c_n)\n",
    "                    shape2 = list(output[1][0].shape)  # Extract h_n only\n",
    "                else:  # RNN/GRU case: h_n only\n",
    "                    shape2 = list(output[1].shape)\n",
    "\n",
    "                # Replace batch dimension (middle position) with -1\n",
    "                shape2[1] = -1\n",
    "\n",
    "                output_shapes[name] = f\"[{shape1}, {shape2}]\"\n",
    "\n",
    "            # Handle standard layer outputs (e.g., Linear)\n",
    "            else:\n",
    "                shape = list(output.shape)\n",
    "                shape[0] = -1  # Replace batch dimension with -1\n",
    "                output_shapes[name] = f\"{shape}\"\n",
    "\n",
    "        return hook\n",
    "\n",
    "    # 1. Determine the device where model parameters reside\n",
    "    try:\n",
    "        device = next(model.parameters()).device\n",
    "    except StopIteration:\n",
    "        device = torch.device(\"cpu\")  # Fallback for models without parameters\n",
    "\n",
    "    # 2. Create dummy input tensors with batch_size=1\n",
    "    dummy_inputs = []\n",
    "    for shape, dtype in input_specs:\n",
    "        if dtype in [torch.long, torch.int, torch.int8, torch.int16, torch.int32, torch.int64]:\n",
    "            dummy_inputs.append(torch.zeros(1, *shape, dtype=dtype).to(device))\n",
    "        else:\n",
    "            dummy_inputs.append(torch.randn(1, *shape, dtype=dtype).to(device))\n",
    "\n",
    "    # 3. Register forward hooks on target layers\n",
    "    # Iterate through direct children of the model (e.g., self.rnn, self.classifier)\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, (nn.Linear, nn.RNN, nn.GRU, nn.LSTM, nn.ModuleList)):\n",
    "            # Register the hook and store its handle for cleanup\n",
    "            hook_handle = module.register_forward_hook(get_hook(name))\n",
    "            hooks.append(hook_handle)\n",
    "\n",
    "    # 4. Execute a dummy forward pass in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            model(*dummy_inputs)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dummy forward pass: {e}\")\n",
    "            # Clean up hooks even if an error occurs\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "            return\n",
    "\n",
    "    # 5. Remove all registered hooks\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "\n",
    "    # --- 6. Print the summary table ---\n",
    "\n",
    "    print(\"-\" * 79)\n",
    "    # Column headers\n",
    "    print(f\"{'Layer (type)':<25} {'Output Shape':<28} {'Param #':<18}\")\n",
    "    print(\"=\" * 79)\n",
    "\n",
    "    total_params = 0\n",
    "    total_trainable_params = 0\n",
    "\n",
    "    # Iterate through modules again to collect and display parameter information\n",
    "    for name, module in model.named_children():\n",
    "        if name in output_shapes:\n",
    "            # Count total and trainable parameters for this module\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "            total_params += module_params\n",
    "            total_trainable_params += trainable_params\n",
    "\n",
    "            # Format strings for display\n",
    "            layer_name = f\"{name} ({type(module).__name__})\"\n",
    "            output_shape_str = str(output_shapes[name])\n",
    "            params_str = f\"{trainable_params:,}\"\n",
    "\n",
    "            print(f\"{layer_name:<25} {output_shape_str:<28} {params_str:<15}\")\n",
    "\n",
    "    print(\"=\" * 79)\n",
    "    print(f\"Total params: {total_params:,}\")\n",
    "    print(f\"Trainable params: {total_trainable_params:,}\")\n",
    "    print(f\"Non-trainable params: {total_params - total_trainable_params:,}\")\n",
    "    print(\"-\" * 79)"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "id": "6e391dae3456274e",
   "metadata": {
    "id": "6e391dae3456274e",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.646164Z",
     "start_time": "2025-11-14T01:31:30.625370Z"
    }
   },
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class RecurrentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Generic RNN classifier with Embedding layer for categorical features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            continuous_input_size,\n",
    "            categorical_cardinalities,\n",
    "            embedding_dims,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            num_classes,\n",
    "            rnn_type='GRU',\n",
    "            bidirectional=False,\n",
    "            dropout_rate=0.2,\n",
    "            use_conv: bool = False,\n",
    "            conv_num_filters: int = 64,\n",
    "            conv_kernel_size: int = 5,\n",
    "            conv_num_layers: int = 1,\n",
    "            conv_stride: int = 1,\n",
    "            conv_pool: Optional[int] = None,\n",
    "            conv_batch_norm: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.use_conv = use_conv\n",
    "        self.conv_num_layers = conv_num_layers\n",
    "        self.conv_kernel_size = conv_kernel_size\n",
    "        self.conv_num_filters = conv_num_filters\n",
    "\n",
    "        # 1. Embedding Layers per le feature categoriche\n",
    "        self.embedding_layers = nn.ModuleList([\n",
    "            nn.Embedding(num_embeddings, emb_dim)\n",
    "            for num_embeddings, emb_dim in zip(categorical_cardinalities, embedding_dims)\n",
    "        ])\n",
    "        total_embedding_dim = sum(embedding_dims)\n",
    "\n",
    "        # 2. Calcola la dimensione dell'input per la RNN\n",
    "        rnn_input_size = continuous_input_size + total_embedding_dim\n",
    "\n",
    "        rnn_map = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}\n",
    "        if rnn_type not in rnn_map:\n",
    "            raise ValueError(\"rnn_type must be 'RNN', 'LSTM', or 'GRU'\")\n",
    "        rnn_module = rnn_map[rnn_type]\n",
    "\n",
    "        dropout_val = dropout_rate if num_layers > 1 else 0\n",
    "\n",
    "        if use_conv:\n",
    "            conv_layers = []\n",
    "            in_channels = rnn_input_size\n",
    "            for layer_idx in range(conv_num_layers):\n",
    "                out_channels = conv_num_filters\n",
    "                conv_layers.append(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        kernel_size=conv_kernel_size,\n",
    "                        stride=conv_stride,\n",
    "                        padding=(conv_kernel_size - 1) // 2,\n",
    "                        padding_mode='zeros',\n",
    "                    )\n",
    "                )\n",
    "                if conv_batch_norm:\n",
    "                    conv_layers.append(nn.BatchNorm1d(out_channels))\n",
    "                conv_layers.append(nn.ReLU())\n",
    "                if conv_pool:\n",
    "                    conv_layers.append(nn.MaxPool1d(kernel_size=conv_pool, stride=conv_pool))\n",
    "                in_channels = out_channels\n",
    "            self.conv_block = nn.Sequential(*conv_layers)\n",
    "            rnn_input_size = in_channels\n",
    "\n",
    "        # 3. Crea il layer ricorrente\n",
    "        self.rnn = rnn_module(\n",
    "            input_size=rnn_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_val\n",
    "        )\n",
    "\n",
    "        classifier_input_size = hidden_size * 2 if self.bidirectional else hidden_size\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x_continuous, x_categorical):\n",
    "        \"\"\"\n",
    "        x_continuous shape: (batch_size, seq_length, num_continuous_features)\n",
    "        x_categorical shape: (batch_size, seq_length, num_categorical_features)\n",
    "        \"\"\"\n",
    "        # 1. Applica gli embedding\n",
    "        embedded_features = []\n",
    "        for i, emb_layer in enumerate(self.embedding_layers):\n",
    "            # Prendi la i-esima feature categorica per tutti i timestep\n",
    "            cat_feature = x_categorical[:, :, i]\n",
    "            embedded_features.append(emb_layer(cat_feature))\n",
    "\n",
    "        # 2. Concatena gli embedding\n",
    "        # embedded_features √® una lista di tensori (batch, seq, emb_dim)\n",
    "        # li concateniamo lungo l'ultima dimensione\n",
    "        x_embedded = torch.cat(embedded_features, dim=-1)\n",
    "\n",
    "        # 3. Concatena le feature continue con quelle embedded\n",
    "        x_combined = torch.cat([x_continuous, x_embedded], dim=-1)\n",
    "\n",
    "        # Convolutional layer\n",
    "        if self.use_conv:\n",
    "            # x_combined: (batch, seq, features) -> permute\n",
    "            x_conv = x_combined.permute(0, 2, 1)  # (batch, features, seq)\n",
    "            x_conv = self.conv_block(x_conv)  # (batch, conv_filters, seq')\n",
    "            # Riporta a (batch, seq', features_conv)\n",
    "            x_processed = x_conv.permute(0, 2, 1)\n",
    "        else:\n",
    "            x_processed = x_combined\n",
    "\n",
    "        # 4. Passa il tensore combinato alla RNN\n",
    "        # rnn_out contiene gli output per ogni timestep\n",
    "        rnn_out, hidden = self.rnn(x_processed)\n",
    "\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            hidden = hidden[0]\n",
    "\n",
    "        if self.bidirectional:\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            hidden_to_classify = torch.cat([hidden[-1, 0, :, :], hidden[-1, 1, :, :]], dim=1)\n",
    "        else:\n",
    "            hidden_to_classify = hidden[-1]\n",
    "\n",
    "        # Originale del prof\n",
    "        logits = self.classifier(hidden_to_classify)\n",
    "\n",
    "        # 5. Proposto da gemini. Usa l'output dell'ultimo timestep per la classificazione\n",
    "        # rnn_out ha shape (batch_size, seq_length, hidden_size * num_directions)\n",
    "        # Prendiamo l'output dell'ultimo timestep: rnn_out[:, -1, :]\n",
    "        # last_timestep_output = rnn_out[:, -1, :]\n",
    "\n",
    "        # 6. Classifica\n",
    "        # logits = self.classifier(last_timestep_output)\n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "id": "42a2e099723b2e2e",
   "metadata": {
    "id": "42a2e099723b2e2e"
   },
   "source": [
    "## üßÆ **Network and Training Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "id": "6886410170bd226d",
   "metadata": {
    "id": "6886410170bd226d",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.657454Z",
     "start_time": "2025-11-14T01:31:30.649696Z"
    }
   },
   "source": [
    "# Cross-validation\n",
    "K = 4  # Number of splits (5 and 10 are considered good values)\n",
    "N_TEST_SAMPLE_INDEXES = int(TEST_SET_PERCENTAGE * len(unique_samples))\n",
    "\n",
    "# Training\n",
    "EPOCHS = 500  # Maximum epochs (increase to improve performance)\n",
    "PATIENCE = 35  # Early stopping patience (increase to improve performance)\n",
    "VERBOSE = 20  # Print frequency\n",
    "\n",
    "# Optimisation\n",
    "LEARNING_RATE = 9e-5  # Learning rate\n",
    "BATCH_SIZE = 512  # Batch size\n",
    "WINDOW_SIZE = 20  # Input window size\n",
    "STRIDE = 10  # Input stride\n",
    "\n",
    "# Architecture\n",
    "HIDDEN_LAYERS = 2  # Hidden layers\n",
    "HIDDEN_SIZE = 64  # Neurons per layer\n",
    "RNN_TYPE = 'LSTM'  # Type of RNN architecture\n",
    "BIDIRECTIONAL = True  # Bidirectional RNN\n",
    "\n",
    "# Regularisation\n",
    "DROPOUT_RATE = 0.7  # Dropout probability\n",
    "L1_LAMBDA = 1e-4  # L1 penalty\n",
    "L2_LAMBDA = 1e-2  # L2 penalty\n",
    "\n",
    "# Label smoothing\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# Gradient Clipping\n",
    "MAX_GRADIENT_NORM = 1.0\n",
    "\n",
    "# Padding Strategy\n",
    "PADDING_STRATEGY = 'adaptive'  # Options: 'adaptive', 'repeat', 'zero'\n",
    "PADDING_LOOKBACK_STEPS = (STRIDE * 2) % WINDOW_SIZE  # Number of timesteps for adaptive padding statistics\n",
    "\n",
    "# Embedding dims for categorical dimensions\n",
    "MIN_N_EMBEDDING_DIMS = 50"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.704827Z",
     "start_time": "2025-11-14T01:31:30.698564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convolution 1d\n",
    "\n",
    "USE_CONV = True\n",
    "CONV_NUM_FILTERS = 64\n",
    "CONV_KERNEL_SIZE = 5\n",
    "CONV_NUM_LAYERS = 1\n",
    "CONV_STRIDE = 1\n",
    "CONV_POOL = None\n",
    "CONV_BATCH_NORM = True"
   ],
   "id": "f81c2c33ecab3a87",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìâ **Learning Rate Scheduler Configuration**\n",
    "\n",
    "Il training ora supporta diversi tipi di learning rate schedulers:\n",
    "\n",
    "**Scheduler disponibili:**\n",
    "- `'reduce_on_plateau'`: Riduce LR quando il metric si stabilizza\n",
    "- `'cosine'`: Cosine Annealing - Riduzione smooth del LR\n",
    "- `'step'`: StepLR - Riduce LR ogni N epochs\n",
    "\n",
    "**Come usare:**\n",
    "1. Imposta `USE_SCHEDULER = True` nella sezione hyperparameters\n",
    "2. Scegli `SCHEDULER_TYPE` (default: 'reduce_on_plateau')\n",
    "3. Configura i parametri specifici dello scheduler se necessario\n",
    "\n",
    "Lo scheduler viene automaticamente integrato nel training loop e il learning rate viene tracciato in TensorBoard."
   ],
   "id": "a4ef0b8dbfa848ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.755291Z",
     "start_time": "2025-11-14T01:31:30.749447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Learning Rate Scheduler\n",
    "USE_SCHEDULER = True  # Enable/disable scheduler\n",
    "SCHEDULER_TYPE = 'cosine'  # Options: 'reduce_on_plateau', 'cosine', 'step'\n",
    "SCHEDULER_PATIENCE = 10  # For ReduceLROnPlateau\n",
    "SCHEDULER_FACTOR = 0.8  # For ReduceLROnPlateau and StepLR\n",
    "SCHEDULER_STEP_SIZE = 30  # For StepLR"
   ],
   "id": "ae7be6ddd7d2553c",
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "id": "fb5d375c8634b150",
   "metadata": {
    "id": "fb5d375c8634b150",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.801423Z",
     "start_time": "2025-11-14T01:31:30.790347Z"
    }
   },
   "source": [
    "# Definiamo le cardinalit√† (numero di valori unici) per ogni feature categorica.\n",
    "# La cardinalit√† deve essere il valore massimo della categoria.\n",
    "# Questo assicura che tutti gli indici siano validi per il layer di embedding.\n",
    "categorical_cardinalities = [\n",
    "    int(df_dataset_reduced['n_legs'].max() + 1),\n",
    "    int(df_dataset_reduced['n_hands'].max() + 1),\n",
    "    int(df_dataset_reduced['n_eyes'].max() + 1),\n",
    "    int(df_dataset_reduced['pain_survey_1'].max() + 1),\n",
    "    int(df_dataset_reduced['pain_survey_2'].max() + 1),\n",
    "    int(df_dataset_reduced['pain_survey_3'].max() + 1),\n",
    "    int(df_dataset_reduced['pain_survey_4'].max() + 1)\n",
    "]\n",
    "\n",
    "# Definiamo la dimensione dell'embedding per ogni feature.\n",
    "embedding_dims = [max(MIN_N_EMBEDDING_DIMS, (c + 1) // 2) for c in categorical_cardinalities]  #= [2,2,2,2,2,2,2]"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "id": "5fd6952bc77c898d",
   "metadata": {
    "id": "5fd6952bc77c898d",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.861693Z",
     "start_time": "2025-11-14T01:31:30.828951Z"
    }
   },
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "EXPERIMENT_NAME = \"lstm_conv1d_simpleNetwork_onlyTraining\"  #spostato qui che mi ero rotto di scorrere #Legittimo bisogno\n",
    "\n",
    "# Set up TensorBoard logging and save model architecture\n",
    "writer = SummaryWriter(\"./\" + logs_dir + \"/\" + EXPERIMENT_NAME)"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "id": "15a61df8bb3c299b",
   "metadata": {
    "id": "15a61df8bb3c299b",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.908152Z",
     "start_time": "2025-11-14T01:31:30.898641Z"
    }
   },
   "source": [
    "# Fixed hyperparameters (not being tuned)\n",
    "fixed_params = {\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'window_size': WINDOW_SIZE,\n",
    "    'stride': STRIDE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'l1_lambda': L1_LAMBDA,\n",
    "    'l2_lambda': L2_LAMBDA,\n",
    "    'rnn_type': RNN_TYPE,\n",
    "    'bidirectional': BIDIRECTIONAL,\n",
    "    'embedding_dims': embedding_dims,\n",
    "    'label_smoothing': LABEL_SMOOTHING,\n",
    "    'continuous_cols': CONTINUOUS_COLS_REDUCED,\n",
    "    'categorical_cols': CATEGORICAL_COLS_REDUCED,\n",
    "    'labels_df': labels_df,\n",
    "    'hidden_layers': HIDDEN_LAYERS,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'dropout_rate': DROPOUT_RATE,\n",
    "\n",
    "    'padding_strategy': PADDING_STRATEGY,\n",
    "    'padding_lookback_steps': PADDING_LOOKBACK_STEPS,\n",
    "\n",
    "    'use_scheduler': USE_SCHEDULER,\n",
    "    'scheduler_patience': SCHEDULER_PATIENCE,\n",
    "    'scheduler_factor': SCHEDULER_FACTOR,\n",
    "    'scheduler_step_size': SCHEDULER_STEP_SIZE,\n",
    "\n",
    "    \"use_conv\": USE_CONV,\n",
    "    \"conv_num_filters\": CONV_NUM_FILTERS,\n",
    "    \"conv_kernel_size\": CONV_KERNEL_SIZE,\n",
    "    \"conv_num_layers\": CONV_NUM_LAYERS,\n",
    "    \"conv_pool\": CONV_POOL,\n",
    "    \"conv_stride\": CONV_STRIDE,\n",
    "    \"conv_batch_norm\": CONV_BATCH_NORM,\n",
    "}\n",
    "\n",
    "# Cross-validation settings\n",
    "cv_params = {\n",
    "    'epochs': EPOCHS,\n",
    "    'device': device,\n",
    "    'k': K,\n",
    "    'n_test_sample_indexes': N_TEST_SAMPLE_INDEXES,\n",
    "    'patience': PATIENCE,\n",
    "    'verbose': VERBOSE,\n",
    "    'seed': SEED,\n",
    "    'evaluation_metric': \"val_f1\",\n",
    "    'mode': 'max',\n",
    "    'restore_best_weights': True,\n",
    "    'writer': writer,\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "id": "587cbc3f9aa12b85",
   "metadata": {
    "id": "587cbc3f9aa12b85"
   },
   "source": [
    "## üß† **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286446591f8f3aaa",
   "metadata": {
    "id": "286446591f8f3aaa"
   },
   "source": [
    "### **Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "id": "d2ff757937e5c33f",
   "metadata": {
    "id": "d2ff757937e5c33f",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.981093Z",
     "start_time": "2025-11-14T01:31:30.968963Z"
    }
   },
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
    "    \"\"\"\n",
    "    Perform one complete training epoch through the entire training dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        l1_lambda (float): Lambda for L1 regularization\n",
    "        l2_lambda (float): Lambda for L2 regularization\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average_loss, f1 score) - Training loss and f1 score for this epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for batch_idx, (inputs_cont, inputs_cat, targets) in enumerate(train_loader):\n",
    "        # Move data to device (GPU/CPU)\n",
    "        inputs_cont, inputs_cat, targets = inputs_cont.to(device), inputs_cat.to(device), targets.to(device)\n",
    "\n",
    "        # Clear gradients from previous step\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Forward pass with mixed precision (if CUDA available)\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=(\n",
    "                device.type == 'cuda')):  # consider to add dtype=torch.float16 to improve speed\n",
    "            logits = model(inputs_cont, inputs_cat)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Add L1 and L2 regularization\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "\n",
    "        # Backward pass with gradient scaling\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # --- Gradient Clipping ---\n",
    "        # Unscale gradients before clipping to avoid clipping scaled gradients\n",
    "        scaler.unscale_(optimizer)\n",
    "        # Clip the gradients to a maximum norm (e.g., 1.0)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=MAX_GRADIENT_NORM)\n",
    "        # --- End of Clipping ---\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Accumulate metrics\n",
    "        running_loss += loss.item() * inputs_cont.size(0)\n",
    "        predictions = logits.argmax(dim=1)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_f1 = f1_score(\n",
    "        np.concatenate(all_targets),\n",
    "        np.concatenate(all_predictions),\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "    return epoch_loss, epoch_f1"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:30.992755Z",
     "start_time": "2025-11-14T01:31:30.985609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_scheduler(optimizer, scheduler_type, train_loader, epochs, **kwargs):\n",
    "    \"\"\"\n",
    "    Create learning rate scheduler based on configuration.\n",
    "\n",
    "    Args:\n",
    "        optimizer: PyTorch optimizer\n",
    "        scheduler_type: Type of scheduler ('reduce_on_plateau', 'cosine', 'step')\n",
    "        train_loader: DataLoader for calculating steps_per_epoch\n",
    "        epochs: Total number of epochs\n",
    "        **kwargs: Additional scheduler-specific parameters\n",
    "\n",
    "    Returns:\n",
    "        scheduler or None\n",
    "    \"\"\"\n",
    "    if scheduler_type == 'reduce_on_plateau':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='max',\n",
    "            factor=kwargs.get('scheduler_factor', 0.5),\n",
    "            patience=kwargs.get('scheduler_patience', 10),\n",
    "        )\n",
    "    elif scheduler_type == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=epochs,\n",
    "            eta_min=kwargs.get('learning_rate', 1e-3) * 0.05\n",
    "        )\n",
    "    elif scheduler_type == 'step':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=kwargs.get('scheduler_step_size', 30),\n",
    "            gamma=kwargs.get('scheduler_factor', 0.5)\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    return scheduler"
   ],
   "id": "6a5b05d08e6d72b4",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:31.004584Z",
     "start_time": "2025-11-14T01:31:30.997016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_metrics_to_tensorboard(writer: SummaryWriter, epoch, train_loss, train_f1, model):\n",
    "    \"\"\"\n",
    "    Log training metrics and model parameters to TensorBoard for visualization.\n",
    "\n",
    "    Args:\n",
    "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
    "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
    "        train_loss (float): Training loss for this epoch\n",
    "        train_f1 (float): Training f1 score for this epoch\n",
    "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
    "\n",
    "    Note:\n",
    "        This function logs scalar metrics (loss/f1 score) and histograms of model\n",
    "        parameters and gradients, which helps monitor training progress and detect\n",
    "        issues like vanishing/exploding gradients.\n",
    "    \"\"\"\n",
    "    # Log scalar metrics\n",
    "    writer.add_scalar('Loss/Training', train_loss, epoch)\n",
    "    writer.add_scalar('F1/Training', train_f1, epoch)\n",
    "\n",
    "    # Log model parameters and gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            # Check if the tensor is not empty before adding a histogram\n",
    "            if param.numel() > 0:\n",
    "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
    "            if param.grad is not None:\n",
    "                # Check if the gradient tensor is not empty before adding a histogram\n",
    "                if param.grad.numel() > 0:\n",
    "                    if param.grad is not None and torch.isfinite(param.grad).all():\n",
    "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)"
   ],
   "id": "9e700ee2136d079f",
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "id": "630fe43b1e6922dc",
   "metadata": {
    "id": "630fe43b1e6922dc",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:31.022678Z",
     "start_time": "2025-11-14T01:31:31.010382Z"
    }
   },
   "source": [
    "def fit(model, train_loader, epochs, criterion, optimizer, scaler, device,\n",
    "        scheduler=None, l1_lambda=0, l2_lambda=0, patience=0, evaluation_metric=\"val_f1\", mode='max',\n",
    "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Train the neural network model on the training data and validate on the validation data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train\n",
    "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
    "        epochs (int): Number of training epochs\n",
    "        criterion (nn.Module): Loss function (e.g., CrossEntropyLoss, MSELoss)\n",
    "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
    "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
    "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
    "        scheduler (optional): Learning rate scheduler (default: None)\n",
    "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
    "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
    "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
    "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_f1\")\n",
    "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'max')\n",
    "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
    "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
    "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
    "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, training_history, best_val_preds_np, best_val_targets_np) -\n",
    "               Trained model, metrics history\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    training_history = {\n",
    "        'train_loss': [],\n",
    "        'train_f1': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "\n",
    "    # Configure early stopping if patience is set\n",
    "    if patience > 0:\n",
    "        patience_counter = 0\n",
    "        best_metric = float('-inf') if mode == 'max' else float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "    print(f\"Training {epochs} epochs...\")\n",
    "\n",
    "    # Main training loop: iterate through epochs\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        # Forward pass through training data, compute gradients, update weights\n",
    "        train_loss, train_f1 = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
    "        )\n",
    "\n",
    "        # Store metrics for plotting and analysis\n",
    "        training_history['train_loss'].append(train_loss)\n",
    "        training_history['train_f1'].append(train_f1)\n",
    "\n",
    "        # Track current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        training_history['learning_rate'].append(current_lr)\n",
    "\n",
    "        # Write metrics to TensorBoard for visualization\n",
    "        if writer is not None:\n",
    "            log_metrics_to_tensorboard(\n",
    "                writer, epoch, train_loss, train_f1, model\n",
    "            )\n",
    "            writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
    "\n",
    "        # Print progress every N epochs or on first epoch\n",
    "        if verbose > 0:\n",
    "            if epoch % verbose == 0 or epoch == 1:\n",
    "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
    "                      f\"Train: Loss={train_loss:.4f}, F1 Score={train_f1:.4f} | \"\n",
    "                      f\"LR={current_lr:.2e}\")\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                # ReduceLROnPlateau needs the metric\n",
    "                #scheduler.step(val_f1 if mode == 'max' else val_loss)\n",
    "                scheduler.step(train_f1 if mode == 'max' else train_loss)\n",
    "                print(\"WARNING - using ReduceOnPlateau requires a validation set\")\n",
    "            else:\n",
    "                # Other schedulers just need epoch\n",
    "                scheduler.step()\n",
    "\n",
    "        # Early stopping logic: monitor metric and save best model\n",
    "        if patience > 0:\n",
    "            current_metric = training_history[evaluation_metric][-1]\n",
    "            is_improvement = (current_metric > best_metric) if mode == 'max' else (current_metric < best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                best_metric = current_metric\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), \"models/\" + experiment_name + '_model.pt')\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "                    break\n",
    "\n",
    "    # Restore best model weights if early stopping was used\n",
    "    if restore_best_weights and patience > 0:\n",
    "        model.load_state_dict(torch.load(\"models/\" + experiment_name + '_model.pt'))\n",
    "        print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
    "\n",
    "    # Save final model if no early stopping\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"models/\" + experiment_name + '_model.pt')\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "\n",
    "    return model, training_history"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "id": "814f404d42804aef",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:31:31.033703Z",
     "start_time": "2025-11-14T01:31:31.027210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_max_score(scores):\n",
    "    \"\"\"\n",
    "    Extract the maximum score from a dictionary of scores.\n",
    "\n",
    "    Args:\n",
    "        scores: Dict with keys like 'split_0', 'split_1', ..., 'mean', 'std'\n",
    "\n",
    "    Returns:\n",
    "        max_score: Maximum score across splits\n",
    "    \"\"\"\n",
    "    split_scores = [v for k, v in scores.items() if k.startswith('split_')]\n",
    "    return max(split_scores) if split_scores else None"
   ],
   "id": "814f404d42804aef",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "id": "f7372398f2c512f9"
   },
   "cell_type": "markdown",
   "source": "## **Training - no validation**",
   "id": "f7372398f2c512f9"
  },
  {
   "cell_type": "code",
   "id": "839e2d5dc96e5b26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "id": "839e2d5dc96e5b26",
    "outputId": "cf992972-0fd1-4b23-80c0-022679f7c40f",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:35:06.540769Z",
     "start_time": "2025-11-14T01:31:31.035463Z"
    }
   },
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "final_params = {**fixed_params, **cv_params}\n",
    "\n",
    "# --- 2. Prepara il dataset di training completo ---\n",
    "\n",
    "\n",
    "# Stratified split su sample_index per mantenere la distribuzione delle label\n",
    "labels_map = labels_df.set_index('sample_index')['label']\n",
    "y_all = np.array([labels_map[sid] for sid in unique_samples], dtype=np.int64)\n",
    "\n",
    "train_ids, test_ids, y_train, y_test = train_test_split(\n",
    "    unique_samples,\n",
    "    y_all,\n",
    "    test_size=N_TEST_SAMPLE_INDEXES,\n",
    "    stratify=y_all,\n",
    "    random_state=SEED + 1,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "n_train_samples = len(train_ids)\n",
    "assert n_train_samples > 0, \"Train set vuoto, riduci val/test\"\n",
    "\n",
    "df_train = df_dataset_reduced[df_dataset_reduced['sample_index'].isin(train_ids)].copy()\n",
    "df_test = df_dataset_reduced[df_dataset_reduced['sample_index'].isin(test_ids)].copy()\n",
    "\n",
    "\n",
    "# Normalizza l'intero dataset di training e salva lo scaler\n",
    "final_scaler = StandardScaler()\n",
    "features_to_normalize = list(set(CONTINUOUS_COLS_REDUCED) - set(COLS_TO_EXCLUDE_FROM_NORMALIZATION))\n",
    "df_train[features_to_normalize] = final_scaler.fit_transform(df_train[features_to_normalize])\n",
    "df_test[features_to_normalize] = final_scaler.transform(df_test[features_to_normalize])\n",
    "joblib.dump(final_scaler, f\"models/{EXPERIMENT_NAME}_final_scaler.pkl\")\n",
    "print(f\"Scaler salvato in: models/{EXPERIMENT_NAME}_final_scaler.pkl\")\n",
    "\n",
    "# Costruisci le sequenze\n",
    "X_train_cont, X_train_cat, y_train = build_sequences(\n",
    "    df_train,\n",
    "    labels_df,\n",
    "    continuous_cols=final_params['continuous_cols'],\n",
    "    categorical_cols=final_params['categorical_cols'],\n",
    "    window=final_params['window_size'],\n",
    "    stride=final_params['stride'],\n",
    "    padding_strategy=final_params['padding_strategy'],\n",
    "    lookback_steps=final_params['padding_lookback_steps']\n",
    ")\n",
    "X_test_cont, X_test_cat, y_test = build_sequences(\n",
    "    df_test,\n",
    "    labels_df,\n",
    "    continuous_cols=final_params['continuous_cols'],\n",
    "    categorical_cols=final_params['categorical_cols'],\n",
    "    window=final_params['window_size'],\n",
    "    stride=final_params['stride'],\n",
    "    padding_strategy=final_params['padding_strategy'],\n",
    "    lookback_steps=final_params['padding_lookback_steps']\n",
    ")\n",
    "\n",
    "# Crea DataLoader\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_cont).float(), torch.from_numpy(X_train_cat).long(), torch.from_numpy(y_train).long())\n",
    "test_ds = TensorDataset(torch.from_numpy(X_test_cont).float(), torch.from_numpy(X_test_cat).long(), torch.from_numpy(y_test).long())\n",
    "\n",
    "train_loader = make_loader(train_ds, batch_size=final_params['batch_size'], shuffle=False, drop_last=False)\n",
    "test_loader = make_loader(test_ds, batch_size=final_params['batch_size'], shuffle=False, drop_last=False)\n",
    "\n",
    "# --- 3. Inizializza e addestra il modello finale ---\n",
    "final_model = RecurrentClassifier(\n",
    "    continuous_input_size=len(final_params['continuous_cols']),\n",
    "    categorical_cardinalities=categorical_cardinalities,\n",
    "    embedding_dims=final_params['embedding_dims'],\n",
    "    hidden_size=final_params['hidden_size'],\n",
    "    num_layers=final_params['hidden_layers'],\n",
    "    num_classes=num_classes,\n",
    "    rnn_type=final_params['rnn_type'],\n",
    "    bidirectional=final_params['bidirectional'],\n",
    "    dropout_rate=final_params['dropout_rate'],\n",
    "    use_conv=final_params.get('use_conv', False),\n",
    "    conv_num_filters=final_params.get('conv_num_filters', 64),\n",
    "    conv_kernel_size=final_params.get('conv_kernel_size', 5),\n",
    "    conv_num_layers=final_params.get('conv_num_layers', 1),\n",
    "    conv_stride=final_params.get('conv_stride', 1),\n",
    "    conv_pool=final_params.get('conv_pool', 2),\n",
    "    conv_batch_norm=final_params.get('conv_batch_norm', True)\n",
    ").to(device)\n",
    "\n",
    "# Ottimizzatore e Criterio\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=final_params['learning_rate'], weight_decay=final_params['l2_lambda'])\n",
    "\n",
    "# Calcolo pesi per class imbalance\n",
    "class_counts = np.bincount(y_train)\n",
    "total_samples = len(y_train)\n",
    "class_weights = total_samples / (len(np.unique(y_train)) * class_counts)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=final_params['label_smoothing'])\n",
    "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# Scegli un numero di epoche basato sui risultati del CV (es. 170)\n",
    "FINAL_EPOCHS = 300\n",
    "\n",
    "# --- INTEGRAZIONE DELLO SCHEDULER ---\n",
    "# Avviso: 'reduce_on_plateau' non √® adatto per il training finale senza validation set.\n",
    "# Se il tuo scheduler migliore √® quello, considera di usare 'cosine' o 'step' per il training finale.\n",
    "if USE_SCHEDULER and SCHEDULER_TYPE == 'reduce_on_plateau':\n",
    "    print(\"Attenzione: 'reduce_on_plateau' non √® adatto al training finale. Lo scheduler verr√† disabilitato.\")\n",
    "    final_scheduler = None\n",
    "elif USE_SCHEDULER:\n",
    "    final_scheduler = create_scheduler(optimizer, SCHEDULER_TYPE, train_loader, **final_params)\n",
    "    print(f\"Scheduler '{SCHEDULER_TYPE}' attivato per il training finale.\")\n",
    "else:\n",
    "    final_scheduler = None\n",
    "    print(\"Nessuno scheduler attivato per il training finale.\")\n",
    "\n",
    "print(\"Inizio addestramento del modello finale...\")\n",
    "for epoch in range(1, FINAL_EPOCHS + 1):\n",
    "    train_loss, train_f1 = train_one_epoch(\n",
    "        final_model, train_loader, criterion, optimizer, scaler, device,\n",
    "        l1_lambda=final_params['l1_lambda'], l2_lambda=final_params['l2_lambda']\n",
    "    )\n",
    "\n",
    "    # Aggiorna lo scheduler alla fine di ogni epoca\n",
    "    if final_scheduler is not None:\n",
    "        final_scheduler.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch}/{FINAL_EPOCHS} | Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "print(\"Addestramento finale completato.\")\n",
    "\n",
    "# --- 4. Salva il modello finale ---\n",
    "torch.save(final_model.state_dict(), f\"models/{EXPERIMENT_NAME}_final_model.pt\")\n",
    "print(f\"Modello finale salvato in: models/{EXPERIMENT_NAME}_final_model.pt\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler salvato in: models/lstm_conv1d_simpleNetwork_onlyTraining_final_scaler.pkl\n",
      "Scheduler 'cosine' attivato per il training finale.\n",
      "Inizio addestramento del modello finale...\n",
      "Epoch 1/300 | Train Loss: 22.1117, Train F1: 0.0192, LR: 0.000090\n",
      "Epoch 10/300 | Train Loss: 19.1375, Train F1: 0.4902, LR: 0.000090\n",
      "Epoch 20/300 | Train Loss: 16.9091, Train F1: 0.6508, LR: 0.000090\n",
      "Epoch 30/300 | Train Loss: 15.5994, Train F1: 0.7634, LR: 0.000089\n",
      "Epoch 40/300 | Train Loss: 14.7356, Train F1: 0.8216, LR: 0.000089\n",
      "Epoch 50/300 | Train Loss: 14.0711, Train F1: 0.8547, LR: 0.000088\n",
      "Epoch 60/300 | Train Loss: 13.5063, Train F1: 0.8837, LR: 0.000087\n",
      "Epoch 70/300 | Train Loss: 12.9837, Train F1: 0.9066, LR: 0.000086\n",
      "Epoch 80/300 | Train Loss: 12.5510, Train F1: 0.9153, LR: 0.000085\n",
      "Epoch 90/300 | Train Loss: 12.0232, Train F1: 0.9361, LR: 0.000083\n",
      "Epoch 100/300 | Train Loss: 11.5378, Train F1: 0.9425, LR: 0.000082\n",
      "Epoch 110/300 | Train Loss: 11.1091, Train F1: 0.9522, LR: 0.000080\n",
      "Epoch 120/300 | Train Loss: 10.7362, Train F1: 0.9557, LR: 0.000078\n",
      "Epoch 130/300 | Train Loss: 10.4515, Train F1: 0.9609, LR: 0.000077\n",
      "Epoch 140/300 | Train Loss: 10.1537, Train F1: 0.9622, LR: 0.000074\n",
      "Epoch 150/300 | Train Loss: 9.8737, Train F1: 0.9689, LR: 0.000072\n",
      "Epoch 160/300 | Train Loss: 9.6564, Train F1: 0.9676, LR: 0.000070\n",
      "Epoch 170/300 | Train Loss: 9.4536, Train F1: 0.9709, LR: 0.000068\n",
      "Epoch 180/300 | Train Loss: 9.2622, Train F1: 0.9739, LR: 0.000065\n",
      "Epoch 190/300 | Train Loss: 9.1139, Train F1: 0.9747, LR: 0.000063\n",
      "Epoch 200/300 | Train Loss: 8.9608, Train F1: 0.9764, LR: 0.000060\n",
      "Epoch 210/300 | Train Loss: 8.8240, Train F1: 0.9786, LR: 0.000058\n",
      "Epoch 220/300 | Train Loss: 8.7014, Train F1: 0.9785, LR: 0.000055\n",
      "Epoch 230/300 | Train Loss: 8.5982, Train F1: 0.9733, LR: 0.000053\n",
      "Epoch 240/300 | Train Loss: 8.4850, Train F1: 0.9789, LR: 0.000050\n",
      "Epoch 250/300 | Train Loss: 8.3779, Train F1: 0.9818, LR: 0.000047\n",
      "Epoch 260/300 | Train Loss: 8.3004, Train F1: 0.9853, LR: 0.000045\n",
      "Epoch 270/300 | Train Loss: 8.2216, Train F1: 0.9856, LR: 0.000042\n",
      "Epoch 280/300 | Train Loss: 8.1360, Train F1: 0.9869, LR: 0.000039\n",
      "Epoch 290/300 | Train Loss: 8.0601, Train F1: 0.9871, LR: 0.000037\n",
      "Epoch 300/300 | Train Loss: 7.9885, Train F1: 0.9887, LR: 0.000034\n",
      "Addestramento finale completato.\n",
      "Modello finale salvato in: models/lstm_conv1d_simpleNetwork_onlyTraining_final_model.pt\n",
      "CPU times: total: 3min 21s\n",
      "Wall time: 3min 35s\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "id": "341580bea37494d1",
   "metadata": {
    "id": "341580bea37494d1"
   },
   "source": [
    "## **Inference on kaggle dataset**"
   ]
  },
  {
   "cell_type": "code",
   "id": "32643d925c26f316",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32643d925c26f316",
    "outputId": "94cbbb00-ffe2-4726-e95d-231a0a8442de",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:35:07.096984Z",
     "start_time": "2025-11-14T01:35:07.011021Z"
    }
   },
   "source": [
    "\n",
    "model = RecurrentClassifier(\n",
    "    continuous_input_size=len(final_params['continuous_cols']),\n",
    "    categorical_cardinalities=categorical_cardinalities,\n",
    "    embedding_dims=final_params['embedding_dims'],\n",
    "    hidden_size=final_params['hidden_size'],\n",
    "    num_layers=final_params['hidden_layers'],\n",
    "    num_classes=num_classes,\n",
    "    rnn_type=final_params['rnn_type'],\n",
    "    bidirectional=final_params['bidirectional'],\n",
    "    dropout_rate=final_params['dropout_rate'],\n",
    "    use_conv=final_params.get('use_conv', False),\n",
    "    conv_num_filters=final_params.get('conv_num_filters', 64),\n",
    "    conv_kernel_size=final_params.get('conv_kernel_size', 5),\n",
    "    conv_num_layers=final_params.get('conv_num_layers', 1),\n",
    "    conv_stride=final_params.get('conv_stride', 1),\n",
    "    conv_pool=final_params.get('conv_pool', 2),\n",
    "    conv_batch_norm=final_params.get('conv_batch_norm', True)\n",
    ").to(device)\n",
    "\n",
    "# 3) Carica i pesi del modello migliore e lo scaler associato\n",
    "model_path = f\"models/{EXPERIMENT_NAME}_final_model.pt\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "scaler_path = f\"models/{EXPERIMENT_NAME}_final_scaler.pkl\"\n",
    "scaler = joblib.load(scaler_path)\n",
    "\n",
    "print(f\"Modello caricato da {model_path}\")\n",
    "print(f\"Scaler caricato da {scaler_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato da models/lstm_conv1d_simpleNetwork_onlyTraining_final_model.pt\n",
      "Scaler caricato da models/lstm_conv1d_simpleNetwork_onlyTraining_final_scaler.pkl\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "id": "9bb894d6b0be0474",
   "metadata": {
    "id": "9bb894d6b0be0474",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:35:07.137992Z",
     "start_time": "2025-11-14T01:35:07.133002Z"
    }
   },
   "source": "submission_path = f\"submissions/{EXPERIMENT_NAME}_final_submission.csv\"",
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "id": "95a5215cfc279e5d",
   "metadata": {
    "id": "95a5215cfc279e5d",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:35:07.207653Z",
     "start_time": "2025-11-14T01:35:07.193607Z"
    }
   },
   "source": [
    "# 4) Funzione per creare le sequenze per l'inferenza (senza etichette)\n",
    "def build_sequences_inference(df, continuous_cols, categorical_cols, window=200, stride=200,\n",
    "                              padding_strategy='adaptive', lookback_steps=10):\n",
    "    \"\"\"\n",
    "    Build sequences for inference with adaptive padding (no labels).\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with time series data\n",
    "        continuous_cols: List of continuous feature columns\n",
    "        categorical_cols: List of categorical feature columns\n",
    "        window: Window size for sequences\n",
    "        stride: Stride for sliding window\n",
    "        padding_strategy: 'adaptive' (mean/mode), 'repeat' (repeat last), or 'zero' (zeros)\n",
    "        lookback_steps: Number of timesteps for computing padding statistics\n",
    "\n",
    "    Returns:\n",
    "        X_continuous, X_categorical, sample_owners (array of sample_index for each window)\n",
    "    \"\"\"\n",
    "    assert window % stride == 0, \"Window must be divisible by stride\"\n",
    "\n",
    "    X_cont, X_cat, owners = [], [], []\n",
    "\n",
    "    # Pre-compute global statistics for fallback (only if adaptive)\n",
    "    if padding_strategy == 'adaptive':\n",
    "        global_cont_mean = df[continuous_cols].mean().values.astype('float32')\n",
    "        global_cat_mode = df[categorical_cols].mode().iloc[0].values.astype('int8')\n",
    "\n",
    "    for sid, g in df.groupby('sample_index'):\n",
    "        cont = g[continuous_cols].values.astype('float32')\n",
    "        cat = g[categorical_cols].values.astype('int64')\n",
    "\n",
    "        pad = (window - (len(cont) % window)) % window\n",
    "\n",
    "        if pad > 0:\n",
    "            if padding_strategy == 'adaptive':\n",
    "                # Use statistics from last timesteps\n",
    "                lookback = min(lookback_steps, len(cont))\n",
    "\n",
    "                if lookback > 0:\n",
    "                    pad_cont_value = np.mean(cont[-lookback:], axis=0, keepdims=True)\n",
    "                    pad_cat_value = np.array([\n",
    "                        np.bincount(cat[-lookback:, i]).argmax()\n",
    "                        for i in range(cat.shape[1])\n",
    "                    ]).reshape(1, -1)\n",
    "                else:\n",
    "                    pad_cont_value = global_cont_mean.reshape(1, -1)\n",
    "                    pad_cat_value = global_cat_mode.reshape(1, -1)\n",
    "\n",
    "                padding_cont = np.repeat(pad_cont_value, pad, axis=0).astype('float32')\n",
    "                padding_cat = np.repeat(pad_cat_value, pad, axis=0).astype('int64')\n",
    "\n",
    "            elif padding_strategy == 'repeat':\n",
    "                if len(cont) > 0:\n",
    "                    padding_cont = np.repeat(cont[-1:], pad, axis=0)\n",
    "                    padding_cat = np.repeat(cat[-1:], pad, axis=0)\n",
    "                else:\n",
    "                    padding_cont = np.zeros((pad, cont.shape[1]), dtype='float32')\n",
    "                    padding_cat = np.zeros((pad, cat.shape[1]), dtype='int64')\n",
    "            else:  # 'zero' or default\n",
    "                padding_cont = np.zeros((pad, cont.shape[1]), dtype='float32')\n",
    "                padding_cat = np.zeros((pad, cat.shape[1]), dtype='int64')\n",
    "\n",
    "            cont = np.concatenate([cont, padding_cont], axis=0)\n",
    "            cat = np.concatenate([cat, padding_cat], axis=0)\n",
    "\n",
    "        # Build windows\n",
    "        i = 0\n",
    "        while i + window <= len(cont):\n",
    "            X_cont.append(cont[i:i + window])\n",
    "            X_cat.append(cat[i:i + window])\n",
    "            owners.append(sid)\n",
    "            i += stride\n",
    "\n",
    "    return (np.asarray(X_cont, dtype=np.float32),\n",
    "            np.asarray(X_cat, dtype=np.int64),\n",
    "            np.asarray(owners, dtype=np.int32))"
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "id": "ef4b4f4460e8a27d",
   "metadata": {
    "id": "ef4b4f4460e8a27d",
    "ExecuteTime": {
     "end_time": "2025-11-14T01:35:07.350032Z",
     "start_time": "2025-11-14T01:35:07.264667Z"
    }
   },
   "source": [
    "# 5) Normalizza il kaggle test con lo scaler CORRETTO (caricato sopra), quello relativo allo split migliore\n",
    "kaggle_test_df_reduced[features_to_normalize] = scaler.transform(kaggle_test_df_reduced[features_to_normalize])"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T01:35:16.076677Z",
     "start_time": "2025-11-14T01:35:07.359047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 6) Costruisci le sequenze per Kaggle test\n",
    "Xk_cont, Xk_cat, owners = build_sequences_inference(\n",
    "    kaggle_test_df_reduced,\n",
    "    continuous_cols=CONTINUOUS_COLS_REDUCED,\n",
    "    categorical_cols=CATEGORICAL_COLS_REDUCED,\n",
    "    window=final_params['window_size'],\n",
    "    stride=final_params['stride'],\n",
    "    padding_strategy=final_params.get('padding_strategy', 'adaptive'),\n",
    "    lookback_steps=final_params.get('padding_lookback_steps', 10)\n",
    ")\n",
    "\n",
    "# 7) Inference sui windows\n",
    "kaggle_ds = TensorDataset(\n",
    "    torch.from_numpy(Xk_cont).float(),\n",
    "    torch.from_numpy(Xk_cat).long()\n",
    ")\n",
    "kaggle_loader = make_loader(kaggle_ds, batch_size=final_params['batch_size'], shuffle=False, drop_last=False)\n",
    "\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for xb_cont, xb_cat in kaggle_loader:\n",
    "        xb_cont = xb_cont.to(device)\n",
    "        xb_cat = xb_cat.to(device)\n",
    "        logits = model(xb_cont, xb_cat)\n",
    "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "\n",
    "all_preds = np.concatenate(all_preds) if len(all_preds) else np.array([], dtype=np.int64)\n",
    "\n",
    "# 8) Aggrega per sample_index (maggioranza)\n",
    "preds_per_sample = {}\n",
    "for sid, p in zip(owners, all_preds):\n",
    "    preds_per_sample.setdefault(int(sid), []).append(int(p))\n",
    "\n",
    "final_idx = {sid: Counter(v).most_common(1)[0][0] for sid, v in preds_per_sample.items()}\n",
    "\n",
    "# 9) Mappa a etichette testuali e crea submission\n",
    "inv_label_map = {0: 'no_pain', 1: 'low_pain', 2: 'high_pain'}\n",
    "submission = pd.DataFrame({\n",
    "    'sample_index': list(final_idx.keys()),\n",
    "    'label': [inv_label_map[int(v)] for v in final_idx.values()]\n",
    "}).sort_values('sample_index', kind='stable')\n",
    "\n",
    "os.makedirs(\"submissions\", exist_ok=True)\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission salvata in {submission_path}\")\n",
    "\n"
   ],
   "id": "2b2fc0bab8e9b0c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission salvata in submissions/lstm_conv1d_simpleNetwork_onlyTraining_final_submission.csv\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T01:35:47.000151Z",
     "start_time": "2025-11-14T01:35:46.984110Z"
    }
   },
   "cell_type": "code",
   "source": "submission.head(2000)",
   "id": "bfeda8327961fe3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      sample_index    label\n",
       "0                0  no_pain\n",
       "1                1  no_pain\n",
       "2                2  no_pain\n",
       "3                3  no_pain\n",
       "4                4  no_pain\n",
       "...            ...      ...\n",
       "1319          1319  no_pain\n",
       "1320          1320  no_pain\n",
       "1321          1321  no_pain\n",
       "1322          1322  no_pain\n",
       "1323          1323  no_pain\n",
       "\n",
       "[1324 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>1319</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>1320</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>1321</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>1322</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>1323</td>\n",
       "      <td>no_pain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1324 rows √ó 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "effede16149ce87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
